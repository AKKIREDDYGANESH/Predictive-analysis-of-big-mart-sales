# -*- coding: utf-8 -*-
"""sales_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QPAxMOJwoxbSn08yd2CMow5HNrAfipiE
"""



"""# Importing Modules"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sb
# Size the plot appropriately for online display



"""# Loading the Data sets"""

types = {'StateHoliday': np.dtype(str)}
train = pd.read_csv("/content/train_v2.csv", parse_dates=[2], nrows=66901, dtype=types)
store = pd.read_csv("/content/store.csv")
train.head()



"""# data validation and cleaning operations"""

not_open = train[(train['Open'] == 0) & (train['Sales'] != 0)] #Identifies rows where stores are not open and sales are not zero.
print("No closed store with sales: " + str(not_open.size == 0)) #Prints whether there are no closed stores with non-zero sales.

no_sales = train[(train['Open'] == 1) & (train['Sales'] <= 0)] #Identifies rows where stores are open and have no sales.
print("No open store with no sales: " + str(no_sales.size == 0)) #Prints whether there are no open stores with zero or negative sales.

train = train.loc[train['Sales'] > 0] #Filters the DataFrame to include only rows where sales are greater than zero.

assert(train[train['Sales'] == 0].size == 0) #Asserts that there are no rows in the filtered DataFrame where sales are equal to zero.

dates = pd.to_datetime(train['Date'], format='mixed').sort_values()
dates = dates.unique()   #It converts the 'Date' column to datetime format, sorts the dates, and retrieves unique values.
start_date = dates[0]
end_date = dates[-1]
print("Start date: ", start_date)   #Then, it identifies the start and end dates from the sorted dates and prints them.
print("End Date: ", end_date)   #afterward, it generates a date range from the start to the end date and checks if all dates in the range match the dates in the DataFrame.
date_range = pd.date_range(start_date, end_date).values
assert(all(dates == date_range))   #If there's any discrepancy, it raises an AssertionError, ensuring data consistency regarding date ranges.

f, ax = plt.subplots(7, sharex=True, sharey=True) #Creates a figure with 7 subplots sharing both x-axis and y-axis.
for i in range(1, 8):
    mask = train[train['DayOfWeek'] == i] #Filters the DataFrame for rows corresponding to the current day of the week.
    ax[i - 1].set_title("Day {0}".format(i)) #Sets the title for each subplot to indicate the day of the week.
    ax[i - 1].scatter(mask['Customers'], mask['Sales'], label=i) #Plots a scatter plot on each subplot showing the relationship between customers and sales for the specific day.

plt.legend()
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()

"""# scatter plot"""

plt.scatter(train['Customers'], train['Sales'], c=train['DayOfWeek'], alpha=0.7, cmap=plt.cm.get_cmap('viridis'))
#showing the relationship between the number of customers and sales, with each point colored according to the day of the week.
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()

for i in ["0", "a", "b", "c"]:
    data = train[train['StateHoliday'] == i]
    if (len(data) == 0):
        continue
    plt.scatter(data['Customers'], data['Sales'], label=i)

plt.legend()
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()
'''
This code iterates over a list of values ["0", "a", "b", "c"],
filters the train DataFrame for rows where the 'StateHoliday' column matches each value,
 and plots a scatter plot for each subset of data. Points are plotted with 'Customers' on the
 x-axis and 'Sales' on the y-axis. If there are no data points for a specific value, the iteration
 continues to the next value. Finally, it adds a legend to differentiate the scatter..'''

for i in [0, 1]:
    data = train[train['SchoolHoliday'] == i]
    if (len(data) == 0):
        continue
    plt.scatter(data['Customers'], data['Sales'], label=i)

plt.legend()
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()
'''This code iterates over a list of values [0, 1], filters
the train DataFrame for rows where the 'SchoolHoliday'
 column matches each value, and plots a scatter plot for each
  subset of data. Points are plotted with 'Customers' on the x-axis
   and 'Sales' on the y-axis. If there are no data points for a specific
   value, the iteration continues to the next value. Finally, it adds a
   legend to differentiate the scatter points representing each value and
 '''

for i in [0, 1]:
    data = train[train['Promo'] == i]
    if (len(data) == 0):
        continue
    plt.scatter(data['Customers'], data['Sales'], label=i)

plt.legend()
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()
'''This code iterates over a list of values [0, 1],
filters the train DataFrame for rows where the 'Promo'
column matches each value, and plots a scatter plot for
each subset of data. Points are plotted with 'Customers'
on the x-axis and 'Sales' on the y-axis. If there are no
data points for a specific value, the iteration continues
to the next value. Finally, it adds a legend to differentiate
the scatter points representing each value and labels the x-axis and y-axis before displaying the plot.'''

"""#Average sales per customer"""

train['SalesPerCustomer'] = train['Sales'] / train['Customers']# Calculate the sales per customer for each entry in the train DataFrame

avg_store = train.groupby('Store')[['Sales', 'Customers', 'SalesPerCustomer']].mean()# Group the train DataFrame by 'Store' and calculate the mean sales, customers, and sales per customer for each store
avg_store.rename(columns=lambda x: 'Avg' + x, inplace=True)# Rename the columns of the avg_store DataFrame to distinguish them as averages
store = pd.merge(avg_store.reset_index(), store, on='Store')# Merge the avg_store DataFrame with the original store DataFrame based on the 'Store' column

store.head()# Display the first few rows of the merged DataFrame to verify the merge operation

for i in ['a', 'b', 'c', 'd']:
    data = store[store['StoreType'] == i]    # Filter the store DataFrame for rows where the 'StoreType' column matches the current type

    if (len(data) == 0):    # Check if there are any data points for the current store type

        continue
    plt.scatter(data['AvgCustomers'], data['AvgSales'], label=i)    # Plot a scatter plot for the current store type with 'AvgCustomers' on the x-axis and 'AvgSales' on the y-axis


plt.legend()# Add a legend to differentiate scatter points representing each store type

plt.xlabel('Average Customers')# Label the x-axis as 'Average Customers' and the y-axis as 'Average Sales'

plt.ylabel('Average Sales')
plt.show()# Display the plot

for i in [0, 1]:
    data = store[store['Promo2'] == i]    # Filter the store DataFrame to include rows where the 'Promo2' column matches the current value 'i'

    if (len(data) == 0):    # Check if there are any data points for the current value of 'Promo2'

        continue        # If there are no data points, skip to the next iteration

    plt.scatter(data['AvgCustomers'], data['AvgSales'], label=i)    # Plot a scatter plot for the current value of 'Promo2' with 'AvgCustomers' on the x-axis and 'AvgSales' on the y-axis


plt.legend()# Add a legend to differentiate scatter points representing each value of 'Promo2'

plt.xlabel('Average Customers')
plt.ylabel('Average Sales')
plt.show()

"""# Filling NaN Values"""

# fill NaN values
store["CompetitionDistance"].fillna(-1)# Fill NaN values in the 'CompetitionDistance' column with -1 (assuming NaN indicates missing data)

plt.scatter(store['CompetitionDistance'], store['AvgSales'])# Plot a scatter plot with 'CompetitionDistance' on the x-axis and 'AvgSales' on the y-axis


plt.xlabel('CompetitionDistance')# Label the x-axis as 'CompetitionDistance' and the y-axis as 'Average Sales'

plt.ylabel('Average Sales')
plt.show()

!pip install pandas
import pandas as pd

!pip install pandas
import pandas as pd



"""# building features for predictive modeling using data"""

def build_features(train, store):
    # Convert string types into integers
    store['StoreType'] = store['StoreType'].astype('category').cat.codes
    store['Assortment'] = store['Assortment'].astype('category').cat.codes
    train["StateHoliday"] = train["StateHoliday"].astype('category').cat.codes

    merged = pd.merge(train, store, on='Store', how='left')

    # remove NaNs
    NaN_replace = 0
    merged.fillna(NaN_replace, inplace=True)

    merged['Year'] = merged.Date.dt.year
    merged['Month'] = merged.Date.dt.month
    merged['Day'] = merged.Date.dt.day
    #merged['Week'] = merged.Date.dt.week
    merged['Week'] = merged.Date.dt.isocalendar().week

    # Number of months that competition has existed for
    merged['MonthsCompetitionOpen'] = \
        12 * (merged['Year'] - merged['CompetitionOpenSinceYear']) + \
        (merged['Month'] - merged['CompetitionOpenSinceMonth'])
    merged.loc[merged['CompetitionOpenSinceYear'] ==
               NaN_replace, 'MonthsCompetitionOpen'] = NaN_replace

    # Define merged
    return merged

"""Data Preparation:
Converts categorical variables ('StoreType', 'Assortment', 'StateHoliday') into integers for model compatibility.
Merges the train and store DataFrames based on the 'Store' column, combining relevant information from both datasets.
Handles missing values by replacing them with 0.
Extracts temporal features such as year, month, day, and week from the 'Date' column.
Calculates the duration of competition existence in months based on the difference between the current date and the competition opening date.
Returns the modified DataFrame with the engineered features.
"""



"""# Splitting the dataset into training and testing sets for machine learning modeling"""

from sklearn.model_selection import train_test_split

# Define features
features = build_features(train, store)

# Select features for training
X = features[['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',
       'StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth',
       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear',
       'Year', 'Month', 'Day', 'Week', 'MonthsCompetitionOpen']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, features['Sales'], test_size=0.15, random_state=10)

"""Define Features:

Calls the build_features function to engineer features from the train and store DataFrames, storing the result in the features variable.

Select Features:

Selects the relevant features from the engineered dataset (features) for training the machine learning model. These features include various attributes such as store ID, day of the week, promotional activities, competition distance, etc.
Split Data:

Splits the dataset (X) and the target variable (features['Sales']) into training and testing sets using the train_test_split function from scikit-learn.
The training set (X_train, y_train) contains 85% of the data, and the testing set (X_test, y_test) contains 15% of the data.
The test_size parameter specifies the proportion of the dataset to include in the testing set.
The random_state parameter sets the seed for random number generation to ensure reproducibility of the split.
"""

def build_features(train, store):
    # Convert string types into integers
    store['StoreType'] = store['StoreType'].astype('category').cat.codes
    store['Assortment'] = store['Assortment'].astype('category').cat.codes
    train["StateHoliday"] = train["StateHoliday"].astype('category').cat.codes

    merged = pd.merge(train, store, on='Store', how='left')

    # remove NaNs
    NaN_replace = 0
    merged.fillna(NaN_replace, inplace=True)

    merged['Year'] = merged.Date.dt.year
    merged['Month'] = merged.Date.dt.month
    merged['Day'] = merged.Date.dt.day
    #merged['Week'] = merged.Date.dt.week
    merged['Week'] = merged.Date.dt.isocalendar().week

    # Number of months that competition has existed for
    merged['MonthsCompetitionOpen'] = \
        12 * (merged['Year'] - merged['CompetitionOpenSinceYear']) + \
        (merged['Month'] - merged['CompetitionOpenSinceMonth'])
    merged.loc[merged['CompetitionOpenSinceYear'] ==
               NaN_replace, 'MonthsCompetitionOpen'] = NaN_replace

    # Calculate additional features
    merged['AvgSales'] = merged['Sales'].rolling(window=30).mean()
    merged['AvgCustomers'] = merged['Customers'].rolling(window=30).mean()
    merged['AvgSalesPerCustomer'] = merged['AvgSales'] / merged['AvgCustomers']

    merged['MedSales'] = merged['Sales'].rolling(window=30).median()
    merged['MedCustomers'] = merged['Customers'].rolling(window=30).median()
    merged

"""**Calculate Rolling Average for Sales:**

Computes the rolling average of sales over a window of 30 days using the rolling method with the mean() function.
**Calculate Rolling Average for Customers:**

Computes the rolling average of customers over a window of 30 days using the rolling method with the mean() function.
**Calculate Average Sales per Customer:**

Divides the rolling average of sales (AvgSales) by the rolling average of customers (AvgCustomers) to get the average sales per customer over the specified window.
**Calculate Rolling Median for Sales:**

Computes the rolling median of sales over a window of 30 days using the rolling method with the median() function.
**Calculate Rolling Median for Customers:**

Computes the rolling median of customers over a window of 30 days using the rolling method with the median() function.

**Display the DataFrame:**

Outputs the modified merged DataFrame with the newly calculated rolling averages, rolling medians, and average sales per customer.
"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer

# Error calculating function according to kaggle
def rmspe(y, y_hat):
    return np.sqrt(np.mean(((y - y_hat) / y) ** 2))

rmpse_scorer = make_scorer(rmspe, greater_is_better = False) # Loss function

def score(model, X_train, y_train, y_test, y_hat):
    score = cross_val_score(model, X_train, y_train, scoring=rmpse_scorer, cv=5)
    print('Mean', score.mean())
    print('Variance', score.var())
    print('RMSPE', rmspe(y_test, y_hat))

def plot_importance(model):
    k = list(zip(X, model.feature_importances_))
    k.sort(key=lambda tup: tup[1])

    labels, vals = zip(*k)

    plt.barh(np.arange(len(X)), vals, align='center')
    plt.yticks(np.arange(len(X)), labels)

"""**Import Required Libraries:**

Imports necessary functions and modules from scikit-learn (cross_val_score) and metrics (make_scorer) to perform cross-validation and define a custom scoring function.
**Define Custom Scoring Function:**

Defines a function named rmspe that calculates the Root Mean Square Percentage Error (RMSPE), a commonly used evaluation metric in Kaggle competitions. It measures the percentage difference between the actual and predicted sales, scaled by the actual sales.

**Create Scorer for Cross-Validation:**

Creates a scorer object rmpse_scorer using the make_scorer function, specifying the rmspe function as the scoring metric. Setting greater_is_better to False indicates that lower values of the scoring function are better (as RMSPE is an error metric).

**Define Evaluation Function:**

Defines a function named score that evaluates the performance of a model using cross-validation and calculates the RMSPE on the test set.
Utilizes cross_val_score to perform 5-fold cross-validation (cv=5) and computes the mean and variance of the cross-validation scores.
Calculates the RMSPE between the actual (y_test) and predicted (y_hat) sales on the test set.
**Define Importance Plotting Function:**

Defines a function named plot_importance that visualizes the feature importance of a model.
Sorts the features and their corresponding importances (feature_importances_) and plots them as horizontal bars.
"""

def build_features(train, store):
    # Convert string types into integers
    store['StoreType'] = store['StoreType'].astype('category').cat.codes
    store['Assortment'] = store['Assortment'].astype('category').cat.codes
    train["StateHoliday"] = train["StateHoliday"].astype('category').cat.codes

    merged = pd.merge(train, store, on='Store', how='left')

    # remove NaNs
    NaN_replace = 0
    merged.fillna(NaN_replace, inplace=True)

    merged['Year'] = merged.Date.dt.year
    merged['Month'] = merged.Date.dt.month
    merged['Day'] = merged.Date.dt.day
    #merged['Week'] = merged.Date.dt.week
    merged['Week'] = merged.Date.dt.isocalendar().week

    # Number of months that competition has existed for
    merged['MonthsCompetitionOpen'] = \
        12 * (merged['Year'] - merged['CompetitionOpenSinceYear']) + \
        (merged['Month'] - merged['CompetitionOpenSinceMonth'])
    merged.loc[merged['CompetitionOpenSinceYear'] ==
               NaN_replace, 'MonthsCompetitionOpen'] = NaN_replace

    #

"""# Decision Tree Regresstion"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer
def rmspe(y, y_hat):
    return np.sqrt(np.mean(((y - y_hat) / y) ** 2))

rmpse_scorer = make_scorer(rmspe, greater_is_better=False)

# Define and train the decision tree model
decision_tree = DecisionTreeRegressor()
decision_tree.fit(X_train, y_train)

def score(model, X_train, y_train, y_test, y_hat):
    score = cross_val_score(model, X_train, y_train, scoring=rmpse_scorer, cv=5)
    print('Mean', score.mean())
    print('Variance', score.var())
    print('RMSPE', rmspe(y_test, y_hat))

y_hat = decision_tree.predict(X_test)
score(decision_tree, X_train, y_train, y_test, y_hat)

def build_features(train, store):
    # Convert string types into integers
    store['StoreType'] = store['StoreType'].astype('category').cat.codes
    store['Assortment'] = store['Assortment'].astype('category').cat.codes
    train["StateHoliday"] = train["StateHoliday"].astype('category').cat.codes

    merged = pd.merge(train, store, on='Store', how='left')

    # remove NaNs
    NaN_replace = 0
    merged.fillna(NaN_replace, inplace=True)

    merged['Year'] = merged.Date.dt.year
    merged['Month'] = merged.Date.dt.month
    merged['Day'] = merged.Date.dt.day
    #merged['Week'] = merged.Date.dt.week
    merged['Week'] = merged.Date.dt.isocalendar().week

    # Number of months that competition has existed for
    merged['MonthsCompetitionOpen'] = \
        12 * (merged['Year'] - merged['CompetitionOpenSinceYear']) + \
        (merged['Month'] - merged['CompetitionOpenSinceMonth'])
    merged.loc[merged['CompetitionOpenSinceYear'] ==
               NaN_replace, 'MonthsCompetitionOpen'] = NaN_replace

    # Calculate additional features
    merged['AvgSales'] = merged['Sales'].rolling(window=30).mean()
    merged['AvgCustomers'] = merged['Customers'].rolling(window=30).mean()
    merged['AvgSalesPerCustomer'] = merged['AvgSales'] / merged['AvgCustomers']

    merged['MedSales'] = merged['Sales'].rolling(window=30).median()
    merged

"""#Random Forest Resgression"""



from sklearn.ensemble import RandomForestRegressor

# Initialize and train the random forest model
randomForest = RandomForestRegressor()
randomForest.fit(X_train, y_train)

# Make predictions
y_hat = randomForest.predict(X_test)

# Evaluate the model
score(randomForest, X_train, y_train, y_test, y_hat)



"""#XGBoost Regression"""

import xgboost as xgb

def rmspe_xg(yhat, y):
    y = np.expm1(y.get_label())
    yhat = np.expm1(yhat)
    return "rmspe", rmspe(y,yhat)

xgboost_tree = xgb.XGBRegressor(
    n_jobs = -1,
    n_estimators = 1000,
    eta = 0.1,
    max_depth = 2,
    min_child_weight = 2,
    subsample = 0.8,
    colsample_bytree = 0.8,
    tree_method = 'exact',
    reg_alpha = 0.05,
    silent = 0,
    random_state = 1023
)
xgboost_tree.fit(X_train, np.log1p(y_train),
                 eval_set = [(X_train, np.log1p(y_train)), (X_test, np.log1p(y_test))])

"""**1.mport XGBoost:**

Imports the XGBoost library as xgb.

**2.Define Custom Evaluation Metric (rmspe_xg):**

Defines a custom evaluation metric named rmspe_xg for XGBoost.
This function takes two arguments: yhat (predicted values) and y (true values).
It transforms the predictions and true values back to their original scale using np.expm1.
Then, it calculates the RMSPE (Root Mean Square Percentage Error) between the true and predicted values using the rmspe function.
The motivation behind defining this custom metric is to provide a more meaningful evaluation of the model's performance, particularly for the problem domain at hand.

**3.Initialize XGBoost Regressor (xgboost_tree):**

Initializes an XGBoost regressor model with specified hyperparameters:
n_estimators: Number of boosting rounds (trees) to build.
eta: Learning rate (shrinkage).
max_depth: Maximum depth of each tree.
min_child_weight: Minimum sum of instance weight (Hessian) needed in a child.
subsample: Subsample ratio of the training instances.
colsample_bytree: Subsample ratio of columns when constructing each tree.
tree_method: Exact tree construction algorithm used.
reg_alpha: L1 regularization term on weights.
silent: Verbosity level.
random_state: Seed for random number generation.
The goal here is to configure a robust XGBoost model with suitable hyperparameters for accurate prediction and generalization on the dataset.

**4.Train XGBoost Model:**

Fits the XGBoost regressor (xgboost_tree) on the training data (X_train and y_train).
Evaluates the model's performance on both the training and testing datasets using log-transformed target values (np.log1p(y_train) and np.log1p(y_test)).
The purpose of training the model is to learn the underlying patterns in the data and make accurate predictions on unseen data.
Overall, the code aims to build and train an XGBoost model while defining a custom evaluation metric tailored to the specific requirements of the problem domain. This allows for effective model training and evaluation with the goal of achieving high performance and robust generalization on unseen data.
"""

print("Note that this is not in percentage, thus not to scale of graphs above")
xgb.plot_importance(xgboost_tree)

def rmspe_exp(y, y_hat):
    return rmspe(np.expm1(y), np.expm1(y_hat))

rmpse_xg_scorer = make_scorer(rmspe_exp, greater_is_better = False) # Loss function

def score(model, X_train, y_train, y_test, y_hat):
    score = cross_val_score(model, X_train, y_train, scoring=rmpse_xg_scorer, cv=5)
    print('Mean', score.mean())
    print('Variance', score.var())
    print('RMSPE', rmspe(y_test, np.expm1(y_hat)))
# Set the index of the DataFrames
X_train.set_index('Store', inplace=True)
X_test.set_index('Store', inplace=True)

# Train the model
xgboost_tree.fit(X_train, np.log1p(y_train),
                 eval_set = [(X_train, np.log1p(y_train)), (X_test, np.log1p(y_test))])

# Make predictions on the test data
y_hat = xgboost_tree.predict(X_test)

# Evaluate the model
score(xgboost_tree, X_train, np.log1p(y_train), y_test, y_hat)

"""**1.Define RMSPE Function on Original Scale (rmspe_exp):**

Defines a function named rmspe_exp that calculates the RMSPE between the true and predicted values on the original scale of the target variable.
It transforms the predictions and true values back to their original scale using np.expm1.
The purpose of this function is to provide an evaluation metric that is interpretable in the context of the original data scale.

**2.Create Scorer for Cross-Validation (rmpse_xg_scorer):**

Creates a scorer object rmpse_xg_scorer using the make_scorer function, specifying the rmspe_exp function as the scoring metric.
Setting greater_is_better to False indicates that lower values of the scoring function are better (as RMSPE is an error metric).

**3.Define Evaluation Function (score):**

Defines a function named score that evaluates the performance of a model using cross-validation and calculates the RMSPE on the test set.
Utilizes cross_val_score to perform 5-fold cross-validation (cv=5) and computes the mean and variance of the cross-validation scores.
Calculates the RMSPE between the actual (y_test) and predicted (y_hat) sales on the original scale of the target variable.   
**4.Set Index of DataFrames:**

Sets the index of the X_train and X_test DataFrames to 'Store'.
This likely prepares the DataFrames for compatibility with the XGBoost model training process.
**5.Train the XGBoost Model:**

Fits the XGBoost regressor (xgboost_tree) on the training data (X_train and y_train), similar to before.
Evaluates the model's performance on both the training and testing datasets using log-transformed target values (np.log1p(y_train) and np.log1p(y_test)).  
**6.Make Predictions and Evaluate the Model:**

Makes predictions (y_hat) on the test data using the trained XGBoost model.
Evaluates the model's performance using the custom score function, which calculates RMSPE on the original scale of the target variable.
Overall, the code aims to provide a comprehensive evaluation of the XGBoost model's performance using RMSPE as the evaluation metric, both on the log-transformed and original scale of the target variable. This allows for meaningful assessment of the model's accuracy and generalization capabilities.
"""



def build_features(test, store):
    # Create the features
    features = pd.DataFrame({'Store': test['Store']})
    features['Customers'] = test['Customers']
    features['Promo'] = test['Promo']
    features['StateHoliday'] = test['StateHoliday']
    features['DayOfWeek'] = test['DayOfWeek']

    # Add the missing features
    missing_features = set(X) - set(features.columns)
    if missing_features:
        features = pd.merge(features, store[list(missing_features)], on='Store', how='left')

    return features

"""
This function build_features creates a DataFrame of features for the test dataset based on the provided test and store DataFrames. It extracts certain columns from the test DataFrame (test) and adds missing features from the store DataFrame (store). Finally, it returns the DataFrame containing the constructed features."""